# ë¼ì´ë¸ŒëŸ¬ë¦¬
import os, re, json, time, ast
import numpy as np

from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.db import connections
from sentence_transformers import SentenceTransformer

# (ì„ íƒ) 1ë‹¨ê³„ ë©”íƒ€ í•¨ìˆ˜: query -> {sql_text, opinion, main, sub}
try:
    from apis.views_api import run_stage1_nl_to_meta
except Exception as e:
    run_stage1_nl_to_meta = None

# ===========================
# ê³µí†µ ìœ í‹¸
# ===========================

def _dictfetchall(cur):
    cols = [c[0] for c in cur.description] if cur.description else []
    return [dict(zip(cols, r)) for r in cur.fetchall()], cols


# SELECT ... FROM panel_records ... ì—ì„œ WHEREë§Œ ì¶”ì¶œ
_WHERE_RE = re.compile(
    r"select\s+\*\s+from\s+panel_records\s*(where\s+.+?)?\s*;?\s*$",
    flags=re.IGNORECASE | re.DOTALL,
)


def _extract_where(sql_text: str) -> str:
    s = (sql_text or "").strip()
    m = _WHERE_RE.search(s)
    if not m:
        return ""
    where = m.group(1) or ""
    # ORDER BY / LIMIT / OFFSET ì œê±°
    where = re.split(r"\b(order\s+by|limit|offset)\b", where, flags=re.IGNORECASE)[0].strip()
    return where


_ALLOWED_COLS = {
    "id","gender","birth","region","subregion","married","nchild","famsize",
    "education_level","job","work","p_income","h_income",
    "owned_products","phone_brand","phone_model",
    "car_ownship","car_manufacturer","car_model",
    "ever_smoked","brand_smoked","brand_smoked_ETC",
    "ever_esmoked","ever_smoked_brand_ETC","ever_alcohol","p_company",
    "loyalty",  # 2ë‹¨ê³„ ì •ë ¬ì— í•„ìš”
}
_COL_RE = re.compile(
    r"\b([a-zA-Z_][a-zA-Z0-9_]*)\b\s*"
    r"(?:=|<>|!=|>=|<=|>|<|in\s*\(|like\b|ilike\b|between\b|is\s+null|is\s+not\s+null)",
    flags=re.IGNORECASE
)


def _columns_from_where(where_sql: str):
    if not where_sql:
        return []
    cols = set()
    for m in _COL_RE.finditer(where_sql):
        c = m.group(1)
        if c.lower() in {"and","or","not","between","is","null"}:
            continue
        if c in _ALLOWED_COLS:
            cols.add(c)
    cols.add("id")
    return sorted(cols)


def _nullish(v) -> bool:
    return v is None or str(v).strip().lower() in ("", "null", "none", "-")


def _vendor_placeholder():
    vendor = connections["default"].vendor  # 'postgresql' | 'sqlite' | 'mysql' ë“±
    return ("?", "?") if vendor == "sqlite" else ("%s", "%s")


def _clean_tag(v: str) -> str:
    s = (v or "").strip()
    if not s:
        return s
    s = re.sub(r'^[\'"\s]+|[\'"\s,;]+$', '', s)
    s = re.sub(r"\s+", " ", s)
    return s


def _split_qids(qids_used: str):
    if not qids_used:
        return []
    parts = [p.strip() for p in str(qids_used).split("|") if p.strip()]
    seen, unique = set(), []
    for p in parts:
        if p not in seen:
            seen.add(p)
            unique.append(p)
    return unique


def _normalize_retrieved_block(retrieved_docs):
    """list/dict ì–´ë–¤ í˜•íƒœë¡œ ì™€ë„ 4ë‹¨ê³„ì—ì„œ ì“°ê¸° í¸í•œ dictë¡œ ì •ê·œí™”"""
    if not retrieved_docs:
        return None
    if isinstance(retrieved_docs, dict):
        return retrieved_docs
    if isinstance(retrieved_docs, list):
        return retrieved_docs[0] if retrieved_docs else None
    return None


# ===========================
# KURE ì„¤ì • / ì„ë² ë”© ìœ í‹¸
# ===========================

KURE_MODEL_PATH = os.getenv("KURE_MODEL_PATH", "nlpai-lab/KURE-v1")
KURE_NORMALIZE  = os.getenv("KURE_NORMALIZE", "false").lower() in ("1", "true", "yes")

KURE_TABLE     = os.getenv("KURE_TABLE", "kure_item_embeddings_v2")
KURE_UID_COL   = os.getenv("KURE_UID_COL", "uid")
KURE_VEC_COL   = os.getenv("KURE_VEC_COL", "vec")
KURE_MAIN_COL  = os.getenv("KURE_MAIN_COL", "main")
KURE_SUB_COL   = os.getenv("KURE_SUB_COL",  "sub")
KURE_QIDS_COL  = os.getenv("KURE_QIDS_COL", "qids_used")

RDB_BASE_COLS = ["id","gender","birth","region","subregion"]

_sentence_model = None
def _get_kure_model():
    global _sentence_model
    if _sentence_model is None:
        try:
            print(f"[INFO] Loading KURE model: {KURE_MODEL_PATH}")
            _sentence_model = SentenceTransformer(KURE_MODEL_PATH, device="cpu")
            print("[INFO] KURE model loaded successfully.")
        except Exception as e:
            raise RuntimeError(f"KURE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return _sentence_model


def _kure_embed(text: str) -> list[float]:
    s = (text or "").strip()
    if not s:
        raise ValueError("ë¹ˆ opinion ì…ë‹ˆë‹¤.")
    model = _get_kure_model()
    vecs = model.encode([s], normalize_embeddings=KURE_NORMALIZE)
    if isinstance(vecs, np.ndarray):
        vec = vecs[0].tolist()
    else:
        vec = list(vecs[0])
    if not vec or len(vec) == 0:
        raise RuntimeError("KURE ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
    return [float(x) for x in vec]


# ===========================
# 3ë‹¨ê³„ í•µì‹¬ ë¡œì§ (RDB + vecdb êµì§‘í•© + ìœ ì‚¬ë„ ì •ë ¬)
# ===========================

def _run_insight_core(
    *,
    sql_text: str,
    opinion: str,
    main: str,
    sub: str,
    where_sql: str,
    limit: int,
    offset: int,
    candidate_cap: int,
):
    """
    insight_filterì˜ í•µì‹¬ë§Œ ë–¼ì–´ë‚¸ í•¨ìˆ˜.
    ë°˜í™˜: (total_count, page_rows(list[dict]), elapsed_str)
    """
    t0 = time.perf_counter()

    # 1ï¸âƒ£ RDB í›„ë³´êµ° id ë¨¼ì € ì¶”ì¶œ
    candidate_ids = None
    if where_sql:
        ids_sql = f"""
            SELECT id
            FROM panel_records
            {where_sql}
            ORDER BY id DESC
            LIMIT %s OFFSET %s
        """
        with connections["default"].cursor() as cur:
            cur.execute(ids_sql, [candidate_cap, 0])
            candidate_ids = tuple(str(r[0]) for r in cur.fetchall())

    if not candidate_ids:
        elapsed = time.perf_counter() - t0
        return 0, [], f"{elapsed:.2f} sec"

    # 2ï¸âƒ£ vecdbì—ì„œ main, subì— í•´ë‹¹í•˜ë©´ì„œ qids_usedê°€ NULLì´ ì•„ë‹Œ UID + vec ê°€ì ¸ì˜¤ê¸°
    qids_sql = f"""
        SELECT {KURE_UID_COL} AS uid, {KURE_QIDS_COL} AS qids, {KURE_VEC_COL} AS vec
        FROM {KURE_TABLE}
        WHERE {KURE_MAIN_COL} = %s
          AND {KURE_SUB_COL}  = %s
          AND {KURE_QIDS_COL} IS NOT NULL
    """
    with connections["vecdb"].cursor() as cur:
        cur.execute(qids_sql, [main, sub])
        vec_rows = cur.fetchall()  # (uid, qids_used, vec)

    if not vec_rows:
        elapsed = time.perf_counter() - t0
        return 0, [], f"{elapsed:.2f} sec"

    # 3ï¸âƒ£ RDB í›„ë³´êµ°ê³¼ uid êµì§‘í•©
    candidate_set = set(candidate_ids)
    vec_filtered = [
        (str(uid), qids, vec) for uid, qids, vec in vec_rows
        if str(uid) in candidate_set
    ]
    if not vec_filtered:
        elapsed = time.perf_counter() - t0
        return 0, [], f"{elapsed:.2f} sec"

    # uid -> vec / qids ë§¤í•‘ + ì „ì²´ q ì»¬ëŸ¼ ì§‘í•©
    uid_to_vec = {}
    uid_to_qids = {}
    qid_union = set()

    for uid, qids, vec in vec_filtered:
        uid_to_vec[uid] = vec
        q_list = _split_qids(qids)
        uid_to_qids[uid] = q_list
        qid_union.update(q_list)

    qid_cols = sorted([q for q in qid_union if re.fullmatch(r"q\d+", q)])

    # 4ï¸âƒ£ RDBì—ì„œ ì´ uidë“¤ì— ëŒ€í•œ íŒ¨ë„ ì •ë³´ + që‹µë³€ ì¡°íšŒ
    where_cols = _columns_from_where(where_sql)
    select_cols = (where_cols or RDB_BASE_COLS) + qid_cols
    # ì¤‘ë³µ ì œê±°
    seen, unique_cols = set(), []
    for c in select_cols:
        if c not in seen:
            seen.add(c)
            unique_cols.append(c)
    select_list = ", ".join(unique_cols) if unique_cols else "id"

    rdb_sql = f"""
        SELECT {select_list}
        FROM panel_records
        WHERE id = ANY(%s::text[])
    """
    with connections["default"].cursor() as cur:
        cur.execute(rdb_sql, [list(uid_to_vec.keys())])
        rdb_rows = cur.fetchall()
        rdb_cols = [c[0] for c in cur.description] if cur.description else []

    # 5ï¸âƒ£ RDB ê¸°ì¤€ìœ¼ë¡œ "qids_usedì— í•´ë‹¹í•˜ëŠ” ë‹µì´ í•˜ë‚˜ë„ ì—†ëŠ” uid" ì œê±°
    col_idx = {c: i for i, c in enumerate(rdb_cols)}
    rows_raw = {}      # uid -> ê¸°ë³¸ row(dict)
    answers_map = {}   # uid -> {q: value}
    valid_uids = []    # ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚° ëŒ€ìƒ uid

    for r in rdb_rows:
        d = {c: r[i] for c, i in col_idx.items()}
        uid = str(d.get("id"))
        q_all = uid_to_qids.get(uid, [])

        answers = {}
        for q in q_all:
            if q in d and d[q] is not None:
                answers[q] = d[q]

        if not answers:
            continue

        rows_raw[uid] = d
        answers_map[uid] = answers
        valid_uids.append(uid)

    if not valid_uids:
        elapsed = time.perf_counter() - t0
        return 0, [], f"{elapsed:.2f} sec"

    # 6ï¸âƒ£ opinion ì„ë² ë”©
    qv = _kure_embed(opinion)
    qv_np = np.array(qv, dtype=np.float32)
    qnorm = np.linalg.norm(qv_np) + 1e-8

    # 7ï¸âƒ£ "ë‹µì´ ìˆëŠ” uid"ë§Œ ëŒ€ìƒìœ¼ë¡œ vec ìœ ì‚¬ë„ ê³„ì‚°
    sim_list = []
    for uid in valid_uids:
        vec = uid_to_vec[uid]
        if isinstance(vec, str):
            try:
                vec_list = ast.literal_eval(vec)
            except Exception:
                continue
        else:
            vec_list = vec

        vec_np = np.array(vec_list, dtype=np.float32)
        vnorm = np.linalg.norm(vec_np) + 1e-8
        sim = float(np.dot(qv_np, vec_np) / (vnorm * qnorm))
        sim_list.append((uid, sim))

    if not sim_list:
        elapsed = time.perf_counter() - t0
        return 0, [], f"{elapsed:.2f} sec"

    # 8ï¸âƒ£ ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ + í˜ì´ì§€ë„¤ì´ì…˜
    sim_list.sort(key=lambda x: x[1], reverse=True)
    uid_ranked = [uid for uid, _ in sim_list]
    total_count = len(uid_ranked)

    uid_page = uid_ranked[offset: offset + limit]
    sim_map = {uid: sim for uid, sim in sim_list}

    # 9ï¸âƒ£ ìµœì¢… rows_out ì¡°ë¦½ (ì—¬ê¸°ì„œ specì˜ "data"ë¡œ ì‚¬ìš©)
    rows_out = []
    for uid in uid_page:
        base = rows_raw[uid].copy()
        base["qids_used"] = list(answers_map[uid].keys())
        base["answers"] = answers_map[uid]
        base["sim"] = sim_map[uid]
        rows_out.append(base)

    elapsed = time.perf_counter() - t0
    return total_count, rows_out, f"{elapsed:.2f} sec"


# ===========================
# ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸: 2ë‹¨ê³„ + 3ë‹¨ê³„ ìë™ ë¶„ê¸°
# ===========================

@csrf_exempt
@require_http_methods(["POST"])
def rdb_gateway(request):

    # ê³µí†µ ê¸°ë³¸ê°’
    sql_text = ""
    opinion = None
    main = None
    sub = None

    # 0) ìš”ì²­ íŒŒì‹±
    try:
        body = json.loads(request.body or "{}")
    except Exception:
        return JsonResponse({"error": "JSON íŒŒì‹± ì‹¤íŒ¨"}, status=400)

    # ì…ë ¥ì€ sql_text ë˜ëŠ” sql ë‘˜ ë‹¤ ì§€ì›
    sql_text_in = (body.get("sql_text") or body.get("sql") or "").strip()
    # queryëŠ” ìì—°ì–´ ê²€ìƒ‰ìš©
    query_in = (body.get("query") or "").strip()

    limit  = int(body.get("limit") or 20)
    offset = int(body.get("offset") or 0)
    candidate_cap = int(body.get("candidate_cap") or 1000)

    retrieved_docs  = body.get("retrieved_docs")
    retrieved_block = body.get("retrieved_block") or _normalize_retrieved_block(retrieved_docs)

    # 1) sql_text / sql ì´ ì˜¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (/search/sql ìš©)
    if sql_text_in:
        sql_text = sql_text_in
        # opinion/main/subë¥¼ ì˜µì…˜ìœ¼ë¡œ í•¨ê»˜ ë°›ì„ ìˆ˜ë„ ìˆìŒ (ìˆ˜ë™ ìˆ˜ì • ì¼€ì´ìŠ¤)
        opinion = body.get("opinion")
        main = body.get("main")
        sub = body.get("sub")

    # 2) queryë§Œ ì˜¤ë©´ 1ë‹¨ê³„ ë©”íƒ€ í•¨ìˆ˜(run_stage1_nl_to_meta) í˜¸ì¶œ (/search/text ìš©)
    elif query_in:
        if run_stage1_nl_to_meta is None:
            return JsonResponse(
                {"error": "ë©”íƒ€ ìƒì„± í•¨ìˆ˜(run_stage1_nl_to_meta)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."},
                status=500,
            )

        try:
            meta = run_stage1_nl_to_meta(query_in)
        except ValueError as e:
            # user_input ë¹„ì—ˆì„ ë•Œ ë“±
            return JsonResponse({"error": str(e)}, status=400)
        except Exception as e:
            return JsonResponse({"error": f"ë©”íƒ€ ìƒì„± í˜¸ì¶œ ì˜¤ë¥˜: {e}"}, status=500)

        sql_text = (meta.get("sql_text") or "").strip()
        opinion = meta.get("opinion")
        main = meta.get("main")
        sub = meta.get("sub")

        if not sql_text:
            return JsonResponse(
                {"error": "ë©”íƒ€ ìƒì„± ê²°ê³¼ì— sql_textê°€ ì—†ìŠµë‹ˆë‹¤.", "meta": meta},
                status=500,
            )
    else:
        # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì˜¤ë¥˜
        return JsonResponse({"error": "sql ë˜ëŠ” query ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."}, status=400)

    # opinion/main/sub ì •ê·œí™”
    opinion_norm = None if _nullish(opinion) else opinion
    main_norm = None if _nullish(main) else _clean_tag(main)
    sub_norm = None if _nullish(sub) else _clean_tag(sub)

    # WHERE ì¶”ì¶œ
    where_sql = _extract_where(sql_text)

    # ğŸ”¸ 3ë‹¨ê³„ ì‚¬ìš© ì—¬ë¶€: opinion ìˆê³  main/subë„ ìˆì–´ì•¼ í•¨
    use_insight = bool(opinion_norm and main_norm and sub_norm)

    # ===========================
    # 3ë‹¨ê³„: opinion ê¸°ë°˜ insight í•„í„°
    # ===========================
    if use_insight:
        try:
            total, data_rows, elapsed_str = _run_insight_core(
                sql_text=sql_text,
                opinion=opinion_norm,
                main=main_norm,
                sub=sub_norm,
                where_sql=where_sql,
                limit=limit,
                offset=offset,
                candidate_cap=candidate_cap,
            )
        except Exception as e:
            return JsonResponse(
                {"error": f"Insight ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {e}"},
                status=500,
            )

        return JsonResponse(
            {
                "sql": sql_text,
                "opinion": opinion_norm,
                "main": main_norm,
                "sub": sub_norm,
                "count": int(total),
                "sql_executed_time": elapsed_str,
                "data": data_rows,
                "retrieved_block": retrieved_block, # ì°¸ì¡°ìë£Œ
            },
            json_dumps_params={"ensure_ascii": False},
        )

    # ===========================
    # 2ë‹¨ê³„: ìˆœìˆ˜ RDB ê²€ìƒ‰ (ê¸°ì¡´ rdb_gateway)
    # ===========================

    where_clause = f" {where_sql}" if where_sql else ""

    # ê¸°ë³¸ SELECT ì»¬ëŸ¼
    select_cols = _columns_from_where(where_sql) or ["id", "gender", "birth", "region", "subregion"]

    # loyalty ê¸°ì¤€ ì •ë ¬ì´ë¯€ë¡œ loyalty ì»¬ëŸ¼ë„ í¬í•¨ë˜ê²Œ ë³´ì¥
    if "loyalty" not in select_cols:
        select_cols.append("loyalty")

    select_list = ", ".join(select_cols)
    lim_ph, off_ph = _vendor_placeholder()

    # loyalty ê¸°ì¤€ ì •ë ¬
    order_by_clause = "ORDER BY loyalty DESC, id DESC"

    page_sql = f"""
        SELECT {select_list}
        FROM panel_records
        {where_clause}
        {order_by_clause}
        LIMIT {lim_ph} OFFSET {off_ph}
    """.strip()

    count_sql = f"""
        SELECT COUNT(*) AS cnt
        FROM panel_records
        {where_clause}
    """.strip()

    try:
        t0 = time.perf_counter()

        with connections["default"].cursor() as cur:
            cur.execute(page_sql, [int(limit), int(offset)])
            rows, cols = _dictfetchall(cur)

        with connections["default"].cursor() as cur:
            cur.execute(count_sql)
            total = cur.fetchone()[0]

        elapsed = time.perf_counter() - t0
        sql_executed_time = f"{elapsed:.2f} sec"

        # SQL ì§ì ‘ ì‹¤í–‰(/search/sql)ì¸ë° opinion ì•ˆ ì˜¨ ê²½ìš° â†’ ëª…ì„¸ì„œì²˜ëŸ¼ N/A ì„¸íŒ…
        if sql_text_in and not query_in and opinion_norm is None:
            opinion_out = "N/A (User-provided SQL)"
            main_out = "N/A"
            sub_out = "N/A"
        else:
            opinion_out = opinion_norm
            main_out = main_norm
            sub_out = sub_norm

        # ê²°ê³¼ ì—†ìŒë„ ìŠ¤í‚¤ë§ˆ ìœ ì§€
        if not rows or total == 0:
            return JsonResponse(
                {
                    "sql": sql_text,
                    "opinion": opinion_out,
                    "main": main_out,
                    "sub": sub_out,
                    "count": 0,
                    "sql_executed_time": sql_executed_time,
                    "data": [],
                },
                json_dumps_params={"ensure_ascii": False},
            )

        # ê²°ê³¼ ìˆìŒ
        return JsonResponse(
            {
                "sql": sql_text,
                "opinion": opinion_out,
                "main": main_out,
                "sub": sub_out,
                "count": int(total),
                "sql_executed_time": sql_executed_time,
                "data": rows,
            },
            json_dumps_params={"ensure_ascii": False},
        )

    except Exception as e:
        # IndexErrorëŠ” 'ê²°ê³¼ ì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬
        if isinstance(e, IndexError):
            return JsonResponse(
                {
                    "sql": sql_text,
                    "opinion": opinion_norm,
                    "main": main_norm,
                    "sub": sub_norm,
                    "count": 0,
                    "sql_executed_time": "0.00 sec",
                    "data": [],
                    "message": "ê²°ê³¼ ì—†ìŒ (IndexError)",
                },
                json_dumps_params={"ensure_ascii": False},
            )

        # ê·¸ ì™¸ ì—ëŸ¬ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        return JsonResponse(
            {
                "error": f"RDB ì‹¤í–‰ ì˜¤ë¥˜: {type(e).__name__}: {e}",
                "sql": sql_text,
                "where": where_sql,
                "select_cols": select_cols,
                "db_vendor": connections["default"].vendor,
            },
            status=500,
        )
